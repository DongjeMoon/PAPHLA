{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_extra = [62, 69, 76, 116, 118, 150, 158]\n",
    "\n",
    "pockets = [[5, 7, 59, 63, 66, 159, 163, 167, 171],\n",
    "           [7, 9, 24, 34, 45, 63, 66, 67, 70, 99],\n",
    "           [9, 70, 73, 74, 97],\n",
    "           [99, 114, 155, 156, 159, 160],\n",
    "           [97, 114, 147, 152, 156],\n",
    "           [77, 80, 81, 84, 95, 123, 143, 146, 147]]\n",
    "\n",
    "pseudo_indices = set([elem for pocket in pockets for elem in pocket] + pseudo_extra)\n",
    "pseudo_indices = np.array(list(pseudo_indices)) - 1\n",
    "pseudo_indices = np.sort(pseudo_indices)\n",
    "\n",
    "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t6_8M_UR50D\")\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f0f05",
   "metadata": {},
   "source": [
    "### Download HLA sequences from IPD-IMGT/HLA (https://www.ebi.ac.uk/ipd/imgt/hla/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "data = pd.read_csv(\"hla_seq_data.csv\")\n",
    "\n",
    "sequence_representations = {}\n",
    "\n",
    "for start_idx in tqdm(range(0, len(data), batch_size), total=len(data) // batch_size):\n",
    "    batch_data = data[start_idx:start_idx + batch_size]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        hla = data[start_idx + i][0]\n",
    "        seq = data[start_idx + i][1]\n",
    "        embeddings = token_representations[i, 1: tokens_len - 1]\n",
    "        if len(seq) < 171:\n",
    "            print(f'{hla} has less than 171 residues')\n",
    "            continue\n",
    "\n",
    "        if seq.startswith('M'):\n",
    "            embeddings = embeddings[24:, :][pseudo_indices, :]\n",
    "        elif seq.startswith('SH'):\n",
    "            embeddings = embeddings[pseudo_indices-1, :]\n",
    "        elif seq.startswith('HS'):\n",
    "            embeddings = embeddings[pseudo_indices-2, :]\n",
    "        elif 'GSHS' in seq:\n",
    "            seq = seq[seq.find('GSHS'):]\n",
    "            embeddings = embeddings[pseudo_indices, :]\n",
    "        elif 'CSHS' in seq:\n",
    "            seq = seq[seq.find('CSHS'):]\n",
    "            embeddings = embeddings[pseudo_indices, :]\n",
    "        elif 'GCHS' in seq:\n",
    "            seq = seq[seq.find('GCHS'):]\n",
    "            embeddings = embeddings[21:, :][pseudo_indices, :]\n",
    "        else:\n",
    "            try:\n",
    "                embeddings = embeddings[pseudo_indices-1, :]\n",
    "            except:\n",
    "                print(f'{hla} does not contain full residues')\n",
    "                continue\n",
    "\n",
    "        sequence_representations[hla] = embeddings.cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "allele_to_index = {}\n",
    "index_to_allele = {}\n",
    "sorted_alleles = sorted(sequence_representations.keys())  # Sort for consistency\n",
    "\n",
    "num_alleles = len(sequence_representations)\n",
    "embedding_shape = (40, 320)\n",
    "full_shape = (num_alleles, *embedding_shape)\n",
    "\n",
    "mmap_array = np.memmap('hla_esm.mmap',\n",
    "                       dtype=np.float64,\n",
    "                       mode='w+',\n",
    "                       shape=full_shape)\n",
    "\n",
    "for idx, allele in enumerate(sorted_alleles):\n",
    "    allele_to_index[allele] = idx\n",
    "    mmap_array[idx] = np.array(sequence_representations[allele]).astype(np.float64)\n",
    "\n",
    "del mmap_array # Flush to disk\n",
    "\n",
    "index_data = {\n",
    "    'hla_to_idx': allele_to_index,\n",
    "    'shape': list(full_shape), \n",
    "    'dtype': 'float64',\n",
    "}\n",
    "\n",
    "with open('hla_esm_index.json', 'w') as f:\n",
    "    json.dump(index_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved embeddings to memory-mapped file: hla_esm.mmap\")\n",
    "print(f\"Shape: {full_shape}\")\n",
    "print(f\"Size: {num_alleles * embedding_shape[0] * embedding_shape[1] * 4 / (1024**2):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
